{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d0e8b0",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§ª Lab 1 â€” Generative Models Foundations: **GAN vs VAE** (PyTorch, MNIST)\n",
    "\n",
    "**Course:** Generative AI (Day 1)  \n",
    "**Lab Length:** ~3 hours  \n",
    "**Goal:** Implement a **GAN** and a **VAE** on MNISTFashion-MNIST, compare their behavior, and reflect on stability vs. latent structure.\n",
    "\n",
    "> âœ… **What you will submit:**  \n",
    "> - A short reflection with generated image grids and comments.\n",
    "\n",
    "### ðŸš¦ Rules\n",
    "- Keep training epochs small if youâ€™re on CPU; you can always re-run with more epochs later.\n",
    "- Cells marked **(Provided)** can be run as-is; cells marked **(TODO)** require your edits.\n",
    "\n",
    "### ðŸ§° Requirements\n",
    "- PyTorch, Torchvision, Matplotlib, TQDM\n",
    "- (Optional) SciPy for a simple FID-like metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9eb12a",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "- Implement a **vanilla GAN**: Generator + Discriminator, adversarial loss, and training loop.\n",
    "- Implement a **Variational Autoencoder (VAE)**: encoder/decoder, **reparameterization trick**, and **ELBO**.\n",
    "- Produce **visualizations**: sample grids, reconstructions, and **latent interpolations**.\n",
    "- Compare GAN vs VAE using a **proxy FID-like** feature distance.\n",
    "- Reflect on stability, mode collapse, and smoothness of latent space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79827b67",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ§­ Tips for Success\n",
    "- Use **[-1, 1]** input range for GANs (Tanh output).  \n",
    "- Start with **small networks**. You can always scale up.  \n",
    "- Inspect **loss curves**, generated images, and reconstructions **frequently**.\n",
    "- If your GAN collapses, try: smaller LR, label smoothing, or tweak BatchNorm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00711ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'psutil'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== (Provided) Setup & Installs =====\n",
    "# If you're in Colab, uncomment the next line to ensure dependencies:\n",
    "# !pip -q install torch torchvision torchaudio matplotlib tqdm scipy\n",
    "\n",
    "import math, os, random, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf88852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (Provided) Helper: visualization & Results Saving =====\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR = \"results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/gan_samples\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/vae_reconstructions\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/vae_samples\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/vae_interpolations\", exist_ok=True)\n",
    "\n",
    "def show_grid(tensor, title=\"\", nrow=4, value_range=(-1,1), save_path=None):\n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, value_range=value_range)\n",
    "    plt.figure(figsize=(6,6)); plt.axis('off'); plt.title(title, fontsize=12)\n",
    "    plt.imshow(grid.permute(1,2,0)); \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# Mode collapse detection: check diversity in generated samples\n",
    "def detect_mode_collapse(samples, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Detect mode collapse by checking if samples are too similar.\n",
    "    Returns: (is_collapsed, diversity_score)\n",
    "    \"\"\"\n",
    "    # Check diversity: compute pairwise L2 distances between samples\n",
    "    samples_flat = samples.view(samples.size(0), -1)\n",
    "    # Normalize\n",
    "    samples_norm = F.normalize(samples_flat, p=2, dim=1)\n",
    "    # Compute pairwise cosine similarities\n",
    "    similarity_matrix = torch.mm(samples_norm, samples_norm.t())\n",
    "    # Average similarity (excluding diagonal)\n",
    "    avg_similarity = (similarity_matrix.sum() - similarity_matrix.trace()) / (len(samples) * (len(samples) - 1))\n",
    "    diversity_score = 1.0 - avg_similarity.item()\n",
    "    \n",
    "    # Mode collapse if samples are too similar\n",
    "    is_collapsed = diversity_score < threshold\n",
    "    \n",
    "    return is_collapsed, diversity_score\n",
    "\n",
    "# Storage for all experiment results\n",
    "all_results = {\n",
    "    'gan': {},\n",
    "    'vae': {}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Data Loading for Both Datasets =====\n",
    "BATCH = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load both datasets\n",
    "datasets_dict = {}\n",
    "for use_fashion, dataset_name in [(False, 'MNIST'), (True, 'FashionMNIST')]:\n",
    "    if use_fashion:\n",
    "        train_ds = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        test_ds  = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    else:\n",
    "        train_ds = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        test_ds  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    datasets_dict[dataset_name] = {\n",
    "        'train_loader': train_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'train_ds': train_ds,\n",
    "        'test_ds': test_ds\n",
    "    }\n",
    "    \n",
    "    # Quick sanity-check visualization\n",
    "    xb, yb = next(iter(train_loader))\n",
    "    show_grid(xb[:16], title=f\"{dataset_name} Real samples (normalized to [-1,1])\", save_path=None)\n",
    "\n",
    "print(f\"âœ“ Loaded datasets: {list(datasets_dict.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627703c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Part 1 â€” **GAN** (Vanilla)  *(~90 minutes)*\n",
    "\n",
    "We will implement a simple GAN (DCGAN-ish) with:\n",
    "- **Generator**: maps `z ~ N(0, I)` to image `xÌ‚`\n",
    "- **Discriminator**: scores real vs fake\n",
    "- **Loss**: Hinge (recommended) **or** BCE (your choice)\n",
    "\n",
    "> **Milestones**\n",
    "> 1) Implement **Generator** & **Discriminator**  \n",
    "> 2) Choose **loss** (hinge recommended), set **optimizers**  \n",
    "> 3) Implement **training loop** and generate sample grids every epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc52424",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (TODO) GAN Architectures =====\n",
    "IMG_CH = 1\n",
    "IMG_H  = 28\n",
    "IMG_W  = 28\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=64, img_ch=IMG_CH):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128*7*7),\n",
    "            nn.BatchNorm1d(128*7*7),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (128, 7, 7)),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, img_ch, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_ch=IMG_CH):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(img_ch, 32, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*4*4, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        f = self.features(x)\n",
    "        logits = self.classifier(f).squeeze(1)\n",
    "        return logits, f\n",
    "\n",
    "print(\"âœ“ GAN architectures defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe56a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (TODO) GAN Losses =====\n",
    "# Option A: Hinge loss (recommended)\n",
    "def d_loss_hinge(real_logits, fake_logits):\n",
    "    return F.relu(1.0 - real_logits).mean() + F.relu(1.0 + fake_logits).mean()\n",
    "\n",
    "def g_loss_hinge(fake_logits):\n",
    "    return -fake_logits.mean()\n",
    "\n",
    "# Option B: BCE \n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "def d_loss_bce(real_logits, fake_logits):\n",
    "    real_t = torch.ones_like(real_logits)\n",
    "    fake_t = torch.zeros_like(fake_logits)\n",
    "    return bce(real_logits, real_t) + bce(fake_logits, fake_t)\n",
    "\n",
    "def g_loss_bce(fake_logits):\n",
    "    real_t = torch.ones_like(fake_logits)\n",
    "    return bce(fake_logits, real_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04082ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== GAN Training - All Experiments for Both Datasets =====\n",
    "# Run all combinations: Dataset Ã— Loss Type Ã— Z_DIM\n",
    "\n",
    "LR = 2e-4\n",
    "betas = (0.5, 0.999)\n",
    "EPOCHS_GAN = 10\n",
    "\n",
    "# Experiment configurations\n",
    "gan_configs = [\n",
    "    {'loss_type': 'hinge', 'z_dim': 64},\n",
    "    {'loss_type': 'hinge', 'z_dim': 32},\n",
    "    {'loss_type': 'hinge', 'z_dim': 128},\n",
    "    {'loss_type': 'bce', 'z_dim': 64},\n",
    "    {'loss_type': 'bce', 'z_dim': 32},\n",
    "    {'loss_type': 'bce', 'z_dim': 128},\n",
    "]\n",
    "\n",
    "# Run experiments for each dataset\n",
    "for dataset_name, data_loaders in datasets_dict.items():\n",
    "    train_loader = data_loaders['train_loader']\n",
    "    test_loader = data_loaders['test_loader']\n",
    "    \n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# DATASET: {dataset_name}\")\n",
    "    print(f\"{'#'*70}\\n\")\n",
    "    \n",
    "    for config in gan_configs:\n",
    "        loss_type = config['loss_type']\n",
    "        z_dim = config['z_dim']\n",
    "        exp_name = f\"{dataset_name}_GAN_{loss_type}_z{z_dim}\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training: {exp_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Initialize models\n",
    "        G = Generator(z_dim=z_dim).to(device)\n",
    "        D = Discriminator().to(device)\n",
    "        opt_G = torch.optim.Adam(G.parameters(), lr=LR, betas=betas)\n",
    "        opt_D = torch.optim.Adam(D.parameters(), lr=LR, betas=betas)\n",
    "        \n",
    "        # Select loss functions\n",
    "        if loss_type == 'hinge':\n",
    "            d_loss_fn = d_loss_hinge\n",
    "            g_loss_fn = g_loss_hinge\n",
    "        else:\n",
    "            d_loss_fn = d_loss_bce\n",
    "            g_loss_fn = g_loss_bce\n",
    "        \n",
    "        # Fixed noise for consistent visualization\n",
    "        fixed_z = torch.randn(16, z_dim, device=device)\n",
    "        \n",
    "        # Store results\n",
    "        all_results['gan'][exp_name] = {\n",
    "            'dataset': dataset_name,\n",
    "            'loss_type': loss_type,\n",
    "            'z_dim': z_dim,\n",
    "            'losses_d': [],\n",
    "            'losses_g': [],\n",
    "            'best_epoch': 0,\n",
    "            'best_samples': None,\n",
    "            'mode_collapse_epochs': [],\n",
    "            'diversity_scores': []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(1, EPOCHS_GAN+1):\n",
    "            G.train(); D.train()\n",
    "            epoch_losses_d = []\n",
    "            epoch_losses_g = []\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f\"[{exp_name}] Epoch {epoch}/{EPOCHS_GAN}\")\n",
    "            for x, _ in pbar:\n",
    "                x = x.to(device)\n",
    "\n",
    "                # (1) Update D\n",
    "                z = torch.randn(x.size(0), z_dim, device=device)\n",
    "                with torch.no_grad():\n",
    "                    x_fake = G(z)\n",
    "                real_logits, _ = D(x)\n",
    "                fake_logits, _ = D(x_fake)\n",
    "                loss_D = d_loss_fn(real_logits, fake_logits)\n",
    "                opt_D.zero_grad(set_to_none=True)\n",
    "                loss_D.backward()\n",
    "                opt_D.step()\n",
    "\n",
    "                # (2) Update G\n",
    "                z = torch.randn(x.size(0), z_dim, device=device)\n",
    "                x_fake = G(z)\n",
    "                fake_logits, _ = D(x_fake)\n",
    "                loss_G = g_loss_fn(fake_logits)\n",
    "                opt_G.zero_grad(set_to_none=True)\n",
    "                loss_G.backward()\n",
    "                opt_G.step()\n",
    "\n",
    "                epoch_losses_d.append(loss_D.item())\n",
    "                epoch_losses_g.append(loss_G.item())\n",
    "                pbar.set_postfix({'D': f\"{loss_D.item():.3f}\", 'G': f\"{loss_G.item():.3f}\"})\n",
    "\n",
    "            # Save epoch losses\n",
    "            avg_loss_d = np.mean(epoch_losses_d)\n",
    "            avg_loss_g = np.mean(epoch_losses_g)\n",
    "            all_results['gan'][exp_name]['losses_d'].append(avg_loss_d)\n",
    "            all_results['gan'][exp_name]['losses_g'].append(avg_loss_g)\n",
    "            \n",
    "            # Generate and check samples\n",
    "            with torch.no_grad():\n",
    "                samples = G(fixed_z).cpu()\n",
    "            \n",
    "            # Check for mode collapse\n",
    "            is_collapsed, diversity_score = detect_mode_collapse(samples)\n",
    "            all_results['gan'][exp_name]['diversity_scores'].append(diversity_score)\n",
    "            \n",
    "            if is_collapsed:\n",
    "                all_results['gan'][exp_name]['mode_collapse_epochs'].append(epoch)\n",
    "                print(f\"  âš ï¸  Mode collapse detected at epoch {epoch} (diversity: {diversity_score:.3f})\")\n",
    "            \n",
    "            save_path = f\"{RESULTS_DIR}/gan_samples/{exp_name}_epoch{epoch}.png\"\n",
    "            title = f\"{exp_name} - Epoch {epoch}\"\n",
    "            if is_collapsed:\n",
    "                title += \" [MODE COLLAPSE]\"\n",
    "            show_grid(samples, title=title, save_path=save_path)\n",
    "            \n",
    "            # Save best samples (from last epoch)\n",
    "            if epoch == EPOCHS_GAN:\n",
    "                all_results['gan'][exp_name]['best_samples'] = samples\n",
    "                all_results['gan'][exp_name]['best_epoch'] = epoch\n",
    "                \n",
    "                # Save models for FID computation\n",
    "                torch.save({\n",
    "                    'G': G.state_dict(),\n",
    "                    'D': D.state_dict(),\n",
    "                    'z_dim': z_dim,\n",
    "                    'loss_type': loss_type\n",
    "                }, f\"{RESULTS_DIR}/models/{exp_name}_final.pth\")\n",
    "        \n",
    "        print(f\"âœ“ Completed: {exp_name}\")\n",
    "        if all_results['gan'][exp_name]['mode_collapse_epochs']:\n",
    "            print(f\"  Mode collapse detected in epochs: {all_results['gan'][exp_name]['mode_collapse_epochs']}\")\n",
    "        print()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All GAN experiments completed for all datasets!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3f4f0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Part 2 â€” **VAE**  *(~60 minutes)*\n",
    "\n",
    "We will implement:\n",
    "- Encoder that outputs **Î¼** and **log ÏƒÂ²**\n",
    "- **Reparameterization trick**: `z = Î¼ + Ïƒ âŠ™ Îµ`\n",
    "- Decoder that reconstructs `xÌ‚`\n",
    "- **ELBO** loss = reconstruction + KL divergence\n",
    "\n",
    "> **Milestones**\n",
    "> 1) Build **VAE module** (encode/reparameterize/decode)  \n",
    "> 2) Implement **loss** (reconstruction + KL)  \n",
    "> 3) Train and visualize reconstructions & random samples  \n",
    "> 4) Do a **latent interpolation** between two test images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97915b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== (TODO) VAE Architecture =====\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent=16):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1),   # 14x14\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),  # 7x7\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.enc_fc_mu  = nn.Linear(64*7*7, latent)\n",
    "        self.enc_fc_log = nn.Linear(64*7*7, latent)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec_fc = nn.Linear(latent, 64*7*7)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Unflatten(1, (64, 7, 7)),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),  # 14x14\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1),  # 28x28\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 1, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.enc(x)\n",
    "        mu = self.enc_fc_mu(h)\n",
    "        logvar = self.enc_fc_log(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = (0.5*logvar).exp()\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.dec_fc(z)\n",
    "        x = self.dec(h)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        xhat = self.decode(z)\n",
    "        return xhat, mu, logvar\n",
    "\n",
    "print(\"âœ“ VAE architecture defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074625ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== VAE Loss & Training - All Experiments for Both Datasets =====\n",
    "# Run all latent dimensions: [16, 8, 32] for both datasets\n",
    "\n",
    "def vae_loss(xhat, x, mu, logvar):\n",
    "    recon = F.l1_loss(xhat, x, reduction='sum') / x.size(0)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    return recon + kl, recon, kl\n",
    "\n",
    "EPOCHS_VAE = 10\n",
    "vae_latent_dims = [16, 8, 32]\n",
    "\n",
    "# Run experiments for each dataset\n",
    "for dataset_name, data_loaders in datasets_dict.items():\n",
    "    train_loader = data_loaders['train_loader']\n",
    "    test_loader = data_loaders['test_loader']\n",
    "    \n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# DATASET: {dataset_name} - VAE Experiments\")\n",
    "    print(f\"{'#'*70}\\n\")\n",
    "    \n",
    "    # Get fixed test samples for reconstruction visualization\n",
    "    fixed_test_samples, _ = next(iter(test_loader))\n",
    "    fixed_test_samples = fixed_test_samples[:16].to(device)\n",
    "    \n",
    "    for latent_dim in vae_latent_dims:\n",
    "        exp_name = f\"{dataset_name}_VAE_latent{latent_dim}\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training: {exp_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Initialize VAE\n",
    "        vae = VAE(latent=latent_dim).to(device)\n",
    "        opt_vae = torch.optim.Adam(vae.parameters(), lr=2e-3)\n",
    "        \n",
    "        # Store results\n",
    "        all_results['vae'][exp_name] = {\n",
    "            'dataset': dataset_name,\n",
    "            'latent_dim': latent_dim,\n",
    "            'losses': [],\n",
    "            'recon_losses': [],\n",
    "            'kl_losses': [],\n",
    "            'best_epoch': 0,\n",
    "            'best_reconstructions': None,\n",
    "            'random_samples': None,\n",
    "            'interpolation': None\n",
    "        }\n",
    "        \n",
    "        for epoch in range(1, EPOCHS_VAE+1):\n",
    "            vae.train()\n",
    "            epoch_losses = []\n",
    "            epoch_recon = []\n",
    "            epoch_kl = []\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f\"[{exp_name}] Epoch {epoch}/{EPOCHS_VAE}\")\n",
    "            for x, _ in pbar:\n",
    "                x = x.to(device)\n",
    "                xhat, mu, logvar = vae(x)\n",
    "                loss, rec, kl = vae_loss(xhat, x, mu, logvar)\n",
    "                opt_vae.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                opt_vae.step()\n",
    "                epoch_losses.append(loss.item())\n",
    "                epoch_recon.append(rec.item())\n",
    "                epoch_kl.append(kl.item())\n",
    "                pbar.set_postfix({'loss': f\"{np.mean(epoch_losses):.2f}\", \n",
    "                                'recon': f\"{np.mean(epoch_recon):.2f}\", \n",
    "                                'kl': f\"{np.mean(epoch_kl):.2f}\"})\n",
    "            \n",
    "            # Save epoch losses\n",
    "            all_results['vae'][exp_name]['losses'].append(np.mean(epoch_losses))\n",
    "            all_results['vae'][exp_name]['recon_losses'].append(np.mean(epoch_recon))\n",
    "            all_results['vae'][exp_name]['kl_losses'].append(np.mean(epoch_kl))\n",
    "            \n",
    "            # Visualize reconstructions\n",
    "            vae.eval()\n",
    "            with torch.no_grad():\n",
    "                xhat, _, _ = vae(fixed_test_samples)\n",
    "            \n",
    "            save_path_recon = f\"{RESULTS_DIR}/vae_reconstructions/{exp_name}_epoch{epoch}.png\"\n",
    "            show_grid(fixed_test_samples.cpu(), title=f\"{exp_name} Inputs - Epoch {epoch}\", save_path=None)\n",
    "            show_grid(xhat.cpu(), title=f\"{exp_name} Reconstructions - Epoch {epoch}\", save_path=save_path_recon)\n",
    "            \n",
    "            # Save best reconstructions\n",
    "            if epoch == EPOCHS_VAE:\n",
    "                all_results['vae'][exp_name]['best_reconstructions'] = xhat.cpu()\n",
    "                all_results['vae'][exp_name]['best_epoch'] = epoch\n",
    "                \n",
    "                # Save model for FID computation\n",
    "                torch.save({\n",
    "                    'vae': vae.state_dict(),\n",
    "                    'latent_dim': latent_dim\n",
    "                }, f\"{RESULTS_DIR}/models/{exp_name}_final.pth\")\n",
    "        \n",
    "        # Generate random samples\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(16, latent_dim, device=device)\n",
    "            random_samples = vae.decode(z).cpu()\n",
    "        \n",
    "        save_path_samples = f\"{RESULTS_DIR}/vae_samples/{exp_name}_random.png\"\n",
    "        show_grid(random_samples, title=f\"{exp_name} Random Samples\", save_path=save_path_samples)\n",
    "        all_results['vae'][exp_name]['random_samples'] = random_samples\n",
    "        \n",
    "        # Latent interpolation\n",
    "        def interpolate(a, b, steps=16):\n",
    "            alphas = torch.linspace(0, 1, steps, device=a.device).view(-1,1)\n",
    "            return (1-alphas)*a + alphas*b\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x, _ = next(iter(test_loader))\n",
    "            x = x.to(device)[:2]\n",
    "            mu, logvar = vae.encode(x)\n",
    "            z1 = mu[0]; z2 = mu[1]\n",
    "            z_traj = interpolate(z1, z2, steps=16)\n",
    "            interp_imgs = vae.decode(z_traj).cpu()\n",
    "        \n",
    "        save_path_interp = f\"{RESULTS_DIR}/vae_interpolations/{exp_name}_interpolation.png\"\n",
    "        show_grid(interp_imgs, title=f\"{exp_name} Latent Interpolation\", save_path=save_path_interp)\n",
    "        all_results['vae'][exp_name]['interpolation'] = interp_imgs\n",
    "        \n",
    "        print(f\"âœ“ Completed: {exp_name}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All VAE experiments completed for all datasets!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Sampling & Interpolation already done in training loop above =====\n",
    "# All VAE outputs (random samples, reconstructions, interpolations) \n",
    "# have been generated and saved during training\n",
    "print(\"VAE sampling and interpolation completed during training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6adcafd",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Part 3 â€” **Comparison & Proxy FID-like Metric**  *(~30 minutes)*\n",
    "\n",
    "We will compare samples from the GAN and VAE using a **feature FrÃ©chet distance** proxy:\n",
    "1) Extract features from the **Discriminator** (penultimate conv layer)\n",
    "2) Fit Gaussians to **real** vs **fake** features\n",
    "3) Compute **FrÃ©chet distance**:  \n",
    "   $\\|\\mu_r-\\mu_f\\|^2 + \\mathrm{Tr}(\\Sigma_r + \\Sigma_f - 2(\\Sigma_r \\Sigma_f)^{1/2})$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> This is not the official FID (which uses Inception), but behaves similarly for quick lab work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Proxy FID-like Metric for All Experiments =====\n",
    "try:\n",
    "    from scipy import linalg\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "    print(\"SciPy not available â€” skipping proxy FID. You can !pip install scipy and re-run.\")\n",
    "\n",
    "def get_features(disc, loader, n_batches=50, use_fake=False, generator=None, z_dim=64, vae=None, latent_dim=16):\n",
    "    disc.eval()\n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(loader):\n",
    "            if i >= n_batches: break\n",
    "            x = x.to(device)\n",
    "            if use_fake:\n",
    "                if generator is not None:\n",
    "                    z = torch.randn(x.size(0), z_dim, device=device)\n",
    "                    x = generator(z)\n",
    "                elif vae is not None:\n",
    "                    z = torch.randn(x.size(0), latent_dim, device=device)\n",
    "                    x = vae.decode(z)\n",
    "            _, f = disc(x)\n",
    "            f = F.adaptive_avg_pool2d(f, 1).flatten(1)  # (B, C)\n",
    "            feats.append(f.cpu())\n",
    "    return torch.cat(feats, dim=0).numpy()\n",
    "\n",
    "def gaussian_stats(X):\n",
    "    mu = X.mean(axis=0)\n",
    "    sigma = np.cov(X, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "def frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        covmean = linalg.sqrtm((sigma1 + np.eye(sigma1.shape[0])*eps).dot(sigma2 + np.eye(sigma2.shape[0])*eps))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2*covmean)\n",
    "    return float(fid)\n",
    "\n",
    "if SCIPY_OK:\n",
    "    print(\"\\nComputing Proxy FID scores for all experiments...\")\n",
    "    \n",
    "    # Compute FID for each dataset separately\n",
    "    for dataset_name, data_loaders in datasets_dict.items():\n",
    "        test_loader = data_loaders['test_loader']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Computing FID for {dataset_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Use a trained discriminator from one of the GAN experiments for this dataset\n",
    "        # Find the first GAN experiment for this dataset to use its discriminator\n",
    "        reference_disc = None\n",
    "        for exp_name in all_results['gan'].keys():\n",
    "            if all_results['gan'][exp_name]['dataset'] == dataset_name:\n",
    "                model_path = f\"{RESULTS_DIR}/models/{exp_name}_final.pth\"\n",
    "                if os.path.exists(model_path):\n",
    "                    checkpoint = torch.load(model_path, map_location=device)\n",
    "                    reference_disc = Discriminator().to(device)\n",
    "                    reference_disc.load_state_dict(checkpoint['D'])\n",
    "                    reference_disc.eval()\n",
    "                    print(f\"Using discriminator from {exp_name} for feature extraction\")\n",
    "                    break\n",
    "        \n",
    "        if reference_disc is None:\n",
    "            print(f\"  Warning: No trained discriminator found for {dataset_name}, skipping FID\")\n",
    "            continue\n",
    "        \n",
    "        # Get real features for this dataset using trained discriminator\n",
    "        print(\"Extracting real image features...\")\n",
    "        real_feats = get_features(reference_disc, test_loader, n_batches=80, use_fake=False)\n",
    "        mu_r, sig_r = gaussian_stats(real_feats)\n",
    "        print(f\"  Real features shape: {real_feats.shape}, mean: {mu_r.shape}\")\n",
    "        \n",
    "        # Compute FID for GAN experiments of this dataset\n",
    "        print(\"\\nComputing FID for GAN experiments...\")\n",
    "        for exp_name in all_results['gan'].keys():\n",
    "            if all_results['gan'][exp_name]['dataset'] != dataset_name:\n",
    "                continue\n",
    "                \n",
    "            # Load saved model\n",
    "            model_path = f\"{RESULTS_DIR}/models/{exp_name}_final.pth\"\n",
    "            if os.path.exists(model_path):\n",
    "                checkpoint = torch.load(model_path, map_location=device)\n",
    "                z_dim = checkpoint['z_dim']\n",
    "                \n",
    "                G_temp = Generator(z_dim=z_dim).to(device)\n",
    "                G_temp.load_state_dict(checkpoint['G'])\n",
    "                G_temp.eval()\n",
    "                \n",
    "                # Use the same trained discriminator for feature extraction\n",
    "                fake_feats = get_features(reference_disc, test_loader, n_batches=80, use_fake=True, \n",
    "                                        generator=G_temp, z_dim=z_dim)\n",
    "                mu_f, sig_f = gaussian_stats(fake_feats)\n",
    "                fid_score = frechet_distance(mu_r, sig_r, mu_f, sig_f)\n",
    "                \n",
    "                all_results['gan'][exp_name]['fid'] = fid_score\n",
    "                print(f\"  {exp_name}: FID = {fid_score:.2f}\")\n",
    "            else:\n",
    "                print(f\"  {exp_name}: Model file not found, skipping FID\")\n",
    "        \n",
    "        # Compute FID for VAE experiments of this dataset\n",
    "        print(\"\\nComputing FID for VAE experiments...\")\n",
    "        for exp_name in all_results['vae'].keys():\n",
    "            if all_results['vae'][exp_name]['dataset'] != dataset_name:\n",
    "                continue\n",
    "                \n",
    "            # Load saved model\n",
    "            model_path = f\"{RESULTS_DIR}/models/{exp_name}_final.pth\"\n",
    "            if os.path.exists(model_path):\n",
    "                checkpoint = torch.load(model_path, map_location=device)\n",
    "                latent_dim = checkpoint['latent_dim']\n",
    "                \n",
    "                vae_temp = VAE(latent=latent_dim).to(device)\n",
    "                vae_temp.load_state_dict(checkpoint['vae'])\n",
    "                vae_temp.eval()\n",
    "                \n",
    "                # Use the same trained discriminator for feature extraction\n",
    "                fake_feats = get_features(reference_disc, test_loader, n_batches=80, use_fake=True, \n",
    "                                        vae=vae_temp, latent_dim=latent_dim)\n",
    "                mu_f, sig_f = gaussian_stats(fake_feats)\n",
    "                fid_score = frechet_distance(mu_r, sig_r, mu_f, sig_f)\n",
    "                \n",
    "                all_results['vae'][exp_name]['fid'] = fid_score\n",
    "                print(f\"  {exp_name}: FID = {fid_score:.2f}\")\n",
    "            else:\n",
    "                print(f\"  {exp_name}: Model file not found, skipping FID\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FID computation completed!\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"SciPy not available. Skipping FID computation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d87ac",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Final Reflection (to submit)\n",
    "\n",
    "1. Copy all the generated outputs, don't forget to label them (e.g   Fashion-MNIST, GAN, Z_DIM=128, EPOCH=... )\n",
    "\n",
    "2. Include image grids:\n",
    "   - GAN samples (best epoch)\n",
    "   - VAE reconstructions\n",
    "   - VAE latent interpolation\n",
    "\n",
    "3. Include your **proxy FID-like** numbers for GAN and VAE.\n",
    "\n",
    "4. Answer briefly:\n",
    "   - What hyperparameters most influenced **GAN stability** in your runs?\n",
    "   - Evidence of **mode collapse** (if any)? What helped?\n",
    "   - How did **latent dim** affect VAE reconstructions and samples?\n",
    "   - One idea to combine benefits of both models (e.g., **VAE-GAN**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Generate Comprehensive Report =====\n",
    "from datetime import datetime\n",
    "\n",
    "report_path = f\"{RESULTS_DIR}/experiment_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"# ðŸ§ª Lab 1 â€” Generative Models: GAN vs VAE - Complete Experiment Report\\n\\n\")\n",
    "    f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    f.write(\"---\\n\\n\")\n",
    "    \n",
    "    # Summary Statistics\n",
    "    f.write(\"## Experiment Summary\\n\\n\")\n",
    "    f.write(f\"- **Datasets:** MNIST, FashionMNIST\\n\")\n",
    "    f.write(f\"- **Total GAN Experiments:** {len(all_results['gan'])}\\n\")\n",
    "    f.write(f\"- **Total VAE Experiments:** {len(all_results['vae'])}\\n\")\n",
    "    f.write(f\"- **Epochs per Experiment:** 10\\n\\n\")\n",
    "    f.write(\"---\\n\\n\")\n",
    "    \n",
    "    # GAN Results by Dataset\n",
    "    for dataset_name in ['MNIST', 'FashionMNIST']:\n",
    "        f.write(f\"## Part 1: GAN Experiments - {dataset_name}\\n\\n\")\n",
    "        f.write(\"### Experiment Configurations\\n\")\n",
    "        f.write(\"- **Loss Types:** Hinge, BCE\\n\")\n",
    "        f.write(\"- **Z Dimensions:** 64, 32, 128\\n\")\n",
    "        f.write(\"- **Total Experiments:** 6\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Results Summary\\n\\n\")\n",
    "        for exp_name, results in all_results['gan'].items():\n",
    "            if results.get('dataset') != dataset_name:\n",
    "                continue\n",
    "            f.write(f\"#### {exp_name}\\n\")\n",
    "            f.write(f\"- **Loss Type:** {results.get('loss_type', 'N/A')}\\n\")\n",
    "            f.write(f\"- **Z Dimension:** {results.get('z_dim', 'N/A')}\\n\")\n",
    "            f.write(f\"- **Best Epoch:** {results['best_epoch']}\\n\")\n",
    "            if results['losses_d']:\n",
    "                f.write(f\"- **Final D Loss:** {results['losses_d'][-1]:.4f}\\n\")\n",
    "                f.write(f\"- **Final G Loss:** {results['losses_g'][-1]:.4f}\\n\")\n",
    "            if 'fid' in results:\n",
    "                f.write(f\"- **Proxy FID Score:** {results['fid']:.2f}\\n\")\n",
    "            if results.get('mode_collapse_epochs'):\n",
    "                f.write(f\"- **âš ï¸ Mode Collapse Detected in Epochs:** {results['mode_collapse_epochs']}\\n\")\n",
    "                if results.get('diversity_scores'):\n",
    "                    f.write(f\"- **Final Diversity Score:** {results['diversity_scores'][-1]:.4f}\\n\")\n",
    "            f.write(f\"- **Sample Images:** `results/gan_samples/{exp_name}_epoch*.png`\\n\\n\")\n",
    "    \n",
    "    # VAE Results by Dataset\n",
    "    for dataset_name in ['MNIST', 'FashionMNIST']:\n",
    "        f.write(f\"## Part 2: VAE Experiments - {dataset_name}\\n\\n\")\n",
    "        f.write(\"### Experiment Configurations\\n\")\n",
    "        f.write(\"- **Latent Dimensions:** 16, 8, 32\\n\")\n",
    "        f.write(\"- **Total Experiments:** 3\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Results Summary\\n\\n\")\n",
    "        for exp_name, results in all_results['vae'].items():\n",
    "            if results.get('dataset') != dataset_name:\n",
    "                continue\n",
    "            f.write(f\"#### {exp_name}\\n\")\n",
    "            f.write(f\"- **Latent Dimension:** {results.get('latent_dim', 'N/A')}\\n\")\n",
    "            f.write(f\"- **Best Epoch:** {results['best_epoch']}\\n\")\n",
    "            if results['losses']:\n",
    "                f.write(f\"- **Final Total Loss:** {results['losses'][-1]:.4f}\\n\")\n",
    "                f.write(f\"- **Final Recon Loss:** {results['recon_losses'][-1]:.4f}\\n\")\n",
    "                f.write(f\"- **Final KL Loss:** {results['kl_losses'][-1]:.4f}\\n\")\n",
    "            if 'fid' in results:\n",
    "                f.write(f\"- **Proxy FID Score:** {results['fid']:.2f}\\n\")\n",
    "            f.write(f\"- **Reconstruction Images:** `results/vae_reconstructions/{exp_name}_epoch*.png`\\n\")\n",
    "            f.write(f\"- **Random Samples:** `results/vae_samples/{exp_name}_random.png`\\n\")\n",
    "            f.write(f\"- **Interpolations:** `results/vae_interpolations/{exp_name}_interpolation.png`\\n\\n\")\n",
    "    \n",
    "    # Image Grid Summary with Embedded Images\n",
    "    f.write(\"## Generated Images\\n\\n\")\n",
    "    f.write(\"All generated images are saved in the `results/` directory. Key images are embedded below:\\n\\n\")\n",
    "    \n",
    "    # GAN Best Samples\n",
    "    f.write(\"### GAN Generated Samples (Best Epoch)\\n\\n\")\n",
    "    for dataset_name in ['MNIST', 'FashionMNIST']:\n",
    "        f.write(f\"#### {dataset_name}\\n\\n\")\n",
    "        for exp_name, results in all_results['gan'].items():\n",
    "            if results.get('dataset') != dataset_name:\n",
    "                continue\n",
    "            best_epoch = results.get('best_epoch', 10)\n",
    "            img_path = f\"{RESULTS_DIR}/gan_samples/{exp_name}_epoch{best_epoch}.png\"\n",
    "            # Use relative path from report location (report is in RESULTS_DIR)\n",
    "            rel_img_path = f\"gan_samples/{exp_name}_epoch{best_epoch}.png\"\n",
    "            if os.path.exists(img_path):\n",
    "                f.write(f\"**{exp_name}** (Epoch {best_epoch}):\\n\\n\")\n",
    "                f.write(f\"![{exp_name}]({rel_img_path})\\n\\n\")\n",
    "    \n",
    "    # VAE Reconstructions\n",
    "    f.write(\"### VAE Reconstructions (Best Epoch)\\n\\n\")\n",
    "    for dataset_name in ['MNIST', 'FashionMNIST']:\n",
    "        f.write(f\"#### {dataset_name}\\n\\n\")\n",
    "        for exp_name, results in all_results['vae'].items():\n",
    "            if results.get('dataset') != dataset_name:\n",
    "                continue\n",
    "            best_epoch = results.get('best_epoch', 10)\n",
    "            img_path = f\"{RESULTS_DIR}/vae_reconstructions/{exp_name}_epoch{best_epoch}.png\"\n",
    "            rel_img_path = f\"vae_reconstructions/{exp_name}_epoch{best_epoch}.png\"\n",
    "            if os.path.exists(img_path):\n",
    "                f.write(f\"**{exp_name}** (Epoch {best_epoch}):\\n\\n\")\n",
    "                f.write(f\"![{exp_name} Reconstructions]({rel_img_path})\\n\\n\")\n",
    "    \n",
    "    # VAE Random Samples\n",
    "    f.write(\"### VAE Random Samples\\n\\n\")\n",
    "    for dataset_name in ['MNIST', 'FashionMNIST']:\n",
    "        f.write(f\"#### {dataset_name}\\n\\n\")\n",
    "        for exp_name, results in all_results['vae'].items():\n",
    "            if results.get('dataset') != dataset_name:\n",
    "                continue\n",
    "            img_path = f\"{RESULTS_DIR}/vae_samples/{exp_name}_random.png\"\n",
    "            rel_img_path = f\"vae_samples/{exp_name}_random.png\"\n",
    "            if os.path.exists(img_path):\n",
    "                f.write(f\"**{exp_name}**:\\n\\n\")\n",
    "                f.write(f\"![{exp_name} Random Samples]({rel_img_path})\\n\\n\")\n",
    "    \n",
    "    # VAE Interpolations\n",
    "    f.write(\"### VAE Latent Interpolations\\n\\n\")\n",
    "    for dataset_name in ['MNIST', 'FashionMNIST']:\n",
    "        f.write(f\"#### {dataset_name}\\n\\n\")\n",
    "        for exp_name, results in all_results['vae'].items():\n",
    "            if results.get('dataset') != dataset_name:\n",
    "                continue\n",
    "            img_path = f\"{RESULTS_DIR}/vae_interpolations/{exp_name}_interpolation.png\"\n",
    "            rel_img_path = f\"vae_interpolations/{exp_name}_interpolation.png\"\n",
    "            if os.path.exists(img_path):\n",
    "                f.write(f\"**{exp_name}**:\\n\\n\")\n",
    "                f.write(f\"![{exp_name} Interpolation]({rel_img_path})\\n\\n\")\n",
    "    \n",
    "    f.write(\"---\\n\\n\")\n",
    "    f.write(\"### All Image Files\\n\\n\")\n",
    "    f.write(\"All generated images are saved in the `results/` directory:\\n\\n\")\n",
    "    f.write(\"- `gan_samples/` - GAN generated samples for each experiment and epoch\\n\")\n",
    "    f.write(\"- `vae_reconstructions/` - VAE reconstruction comparisons\\n\")\n",
    "    f.write(\"- `vae_samples/` - VAE random samples from latent space\\n\")\n",
    "    f.write(\"- `vae_interpolations/` - VAE latent space interpolations\\n\")\n",
    "    f.write(\"- `models/` - Saved model checkpoints for FID computation\\n\\n\")\n",
    "    \n",
    "    # Mode Collapse Analysis\n",
    "    f.write(\"## Mode Collapse Analysis\\n\\n\")\n",
    "    f.write(\"> **Note:** You can answer all reflection questions regardless of whether mode collapse occurs.\\n\")\n",
    "    f.write(\"> - If mode collapse occurs: analyze which configurations caused it and what helped.\\n\")\n",
    "    f.write(\"> - If mode collapse doesn't occur: note which configurations were stable and why.\\n\\n\")\n",
    "    \n",
    "    f.write(\"### GAN Mode Collapse Summary\\n\\n\")\n",
    "    mode_collapse_found = False\n",
    "    for exp_name, results in all_results['gan'].items():\n",
    "        if results.get('mode_collapse_epochs'):\n",
    "            mode_collapse_found = True\n",
    "            f.write(f\"- **{exp_name}**: Mode collapse in epochs {results['mode_collapse_epochs']}\\n\")\n",
    "            if results.get('diversity_scores'):\n",
    "                f.write(f\"  - Diversity scores: {[f'{s:.3f}' for s in results['diversity_scores']]}\\n\")\n",
    "    if not mode_collapse_found:\n",
    "        f.write(\"- **No mode collapse detected** in any GAN experiments.\\n\")\n",
    "        f.write(\"- All experiments maintained good diversity throughout training.\\n\")\n",
    "        f.write(\"- This suggests the hyperparameters (loss type, Z_DIM, learning rate) were well-tuned.\\n\")\n",
    "    f.write(\"\\n### Stable GAN Configurations (No Mode Collapse)\\n\\n\")\n",
    "    for exp_name, results in all_results['gan'].items():\n",
    "        if not results.get('mode_collapse_epochs'):\n",
    "            f.write(f\"- **{exp_name}** (Loss: {results.get('loss_type', 'N/A')}, Z_DIM: {results.get('z_dim', 'N/A')})\\n\")\n",
    "            if results.get('diversity_scores'):\n",
    "                f.write(f\"  - Final diversity score: {results['diversity_scores'][-1]:.4f}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    # FID Scores Summary\n",
    "    f.write(\"## Proxy FID Scores Summary\\n\\n\")\n",
    "    f.write(\"### GAN FID Scores\\n\\n\")\n",
    "    for exp_name, results in all_results['gan'].items():\n",
    "        if 'fid' in results:\n",
    "            f.write(f\"- **{exp_name}**: {results['fid']:.2f}\\n\")\n",
    "    f.write(\"\\n### VAE FID Scores\\n\\n\")\n",
    "    for exp_name, results in all_results['vae'].items():\n",
    "        if 'fid' in results:\n",
    "            f.write(f\"- **{exp_name}**: {results['fid']:.2f}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    # Reflection Questions with Data-Driven Answers\n",
    "    f.write(\"## Reflection Questions & Data-Driven Answers\\n\\n\")\n",
    "    \n",
    "    f.write(\"### 1. What hyperparameters most influenced GAN stability in your runs?\\n\\n\")\n",
    "    f.write(\"**Analysis Guide:**\\n\")\n",
    "    f.write(\"- Compare loss curves (D loss and G loss) across different configurations\\n\")\n",
    "    f.write(\"- Compare FID scores: lower FID = better quality\\n\")\n",
    "    f.write(\"- Compare diversity scores: higher = more diverse samples\\n\")\n",
    "    f.write(\"- Look for patterns:\\n\")\n",
    "    f.write(\"  - Does hinge loss perform better than BCE?\\n\")\n",
    "    f.write(\"  - Does larger Z_DIM (128) help or hurt?\\n\")\n",
    "    f.write(\"  - Are there differences between MNIST and FashionMNIST?\\n\\n\")\n",
    "    \n",
    "    # Add stability analysis\n",
    "    f.write(\"**Stability Indicators from Your Experiments:**\\n\\n\")\n",
    "    for dataset_name in ['MNIST', 'FashionMNIST']:\n",
    "        f.write(f\"**{dataset_name}:**\\n\")\n",
    "        stable_configs = []\n",
    "        unstable_configs = []\n",
    "        for exp_name, results in all_results['gan'].items():\n",
    "            if results.get('dataset') != dataset_name:\n",
    "                continue\n",
    "            if results.get('mode_collapse_epochs'):\n",
    "                unstable_configs.append((exp_name, results.get('loss_type'), results.get('z_dim')))\n",
    "            else:\n",
    "                stable_configs.append((exp_name, results.get('loss_type'), results.get('z_dim')))\n",
    "        \n",
    "        if stable_configs:\n",
    "            f.write(f\"- Stable configurations (no mode collapse): {len(stable_configs)}\\n\")\n",
    "            for exp_name, loss_type, z_dim in stable_configs:\n",
    "                f.write(f\"  - {loss_type} loss, Z_DIM={z_dim}\\n\")\n",
    "        if unstable_configs:\n",
    "            f.write(f\"- Unstable configurations (mode collapse detected): {len(unstable_configs)}\\n\")\n",
    "            for exp_name, loss_type, z_dim in unstable_configs:\n",
    "                f.write(f\"  - {loss_type} loss, Z_DIM={z_dim}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"### 2. Evidence of mode collapse (if any)? What helped?\\n\\n\")\n",
    "    f.write(\"**Answer this question using:**\\n\")\n",
    "    f.write(\"- The Mode Collapse Summary section above\\n\")\n",
    "    f.write(\"- Visual inspection of generated samples (check for lack of diversity)\\n\")\n",
    "    f.write(\"- Diversity scores: lower scores indicate less diversity\\n\\n\")\n",
    "    \n",
    "    if mode_collapse_found:\n",
    "        f.write(\"**Mode Collapse Detected:**\\n\")\n",
    "        f.write(\"- Review which configurations experienced mode collapse\\n\")\n",
    "        f.write(\"- Compare with stable configurations to identify what helped\\n\")\n",
    "        f.write(\"- Common fixes: adjust learning rate, use different loss function, change Z_DIM\\n\\n\")\n",
    "    else:\n",
    "        f.write(\"**No Mode Collapse Detected:**\\n\")\n",
    "        f.write(\"- All experiments maintained good diversity\\n\")\n",
    "        f.write(\"- This suggests your hyperparameters were well-chosen\\n\")\n",
    "        f.write(\"- You can note: \\\"No mode collapse observed. Stable configurations included [list stable ones]\\\"\\n\\n\")\n",
    "    \n",
    "    f.write(\"### 3. How did latent dim affect VAE reconstructions and samples?\\n\\n\")\n",
    "    f.write(\"**Analysis Guide:**\\n\")\n",
    "    f.write(\"- Compare reconstruction losses across latent dimensions (8, 16, 32)\\n\")\n",
    "    f.write(\"- Compare KL divergence: higher = more regularization\\n\")\n",
    "    f.write(\"- Visual inspection: look at reconstruction quality and random sample quality\\n\")\n",
    "    f.write(\"- FID scores: lower = better sample quality\\n\\n\")\n",
    "    \n",
    "    f.write(\"**Latent Dimension Comparison:**\\n\\n\")\n",
    "    for dataset_name in ['MNIST', 'FashionMNIST']:\n",
    "        f.write(f\"**{dataset_name}:**\\n\")\n",
    "        for latent_dim in [8, 16, 32]:\n",
    "            exp_name = f\"{dataset_name}_VAE_latent{latent_dim}\"\n",
    "            if exp_name in all_results['vae']:\n",
    "                results = all_results['vae'][exp_name]\n",
    "                f.write(f\"- Latent dim {latent_dim}:\\n\")\n",
    "                if results.get('losses'):\n",
    "                    f.write(f\"  - Final total loss: {results['losses'][-1]:.4f}\\n\")\n",
    "                if results.get('recon_losses'):\n",
    "                    f.write(f\"  - Final recon loss: {results['recon_losses'][-1]:.4f}\\n\")\n",
    "                if results.get('kl_losses'):\n",
    "                    f.write(f\"  - Final KL loss: {results['kl_losses'][-1]:.4f}\\n\")\n",
    "                if 'fid' in results:\n",
    "                    f.write(f\"  - FID score: {results['fid']:.2f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"**Expected patterns:**\\n\")\n",
    "    f.write(\"- Lower latent dim (8): May have higher reconstruction error, but stronger regularization\\n\")\n",
    "    f.write(\"- Higher latent dim (32): Better reconstruction, but may have less structured latent space\\n\")\n",
    "    f.write(\"- Medium (16): Often a good balance\\n\\n\")\n",
    "    \n",
    "    f.write(\"### 4. One idea to combine benefits of both models (e.g., VAE-GAN)\\n\\n\")\n",
    "    f.write(\"**Consider these approaches:**\\n\")\n",
    "    f.write(\"- **VAE-GAN**: Use VAE encoder to provide structured latent space, GAN discriminator for sharpness\\n\")\n",
    "    f.write(\"- **Adversarial VAE**: Add discriminator to VAE to improve sample quality\\n\")\n",
    "    f.write(\"- **Latent GAN**: Train GAN in VAE's latent space instead of raw noise\\n\")\n",
    "    f.write(\"- **Hybrid training**: Use VAE for reconstruction, GAN for generation\\n\\n\")\n",
    "    f.write(\"**Your observations:**\\n\")\n",
    "    f.write(\"- GAN strengths: Sharp, high-quality samples (when stable)\\n\")\n",
    "    f.write(\"- VAE strengths: Structured latent space, smooth interpolations, reconstruction capability\\n\")\n",
    "    f.write(\"- Combination idea: [Write your idea here based on your results]\\n\\n\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Report saved to: {report_path}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Also create a summary text file\n",
    "summary_path = f\"{RESULTS_DIR}/experiment_summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"EXPERIMENT SUMMARY\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"Datasets: MNIST, FashionMNIST\\n\")\n",
    "    f.write(f\"GAN Experiments: {len(all_results['gan'])}\\n\")\n",
    "    for exp_name in sorted(all_results['gan'].keys()):\n",
    "        f.write(f\"  - {exp_name}\\n\")\n",
    "    f.write(f\"\\nVAE Experiments: {len(all_results['vae'])}\\n\")\n",
    "    for exp_name in sorted(all_results['vae'].keys()):\n",
    "        f.write(f\"  - {exp_name}\\n\")\n",
    "    f.write(f\"\\nAll outputs saved in: {RESULTS_DIR}/\\n\")\n",
    "\n",
    "print(f\"Summary saved to: {summary_path}\")\n",
    "print(\"\\nAll experiments completed! Check the 'results/' directory for all outputs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf16395",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab 1 â€” Generative Models Foundations: GAN vs VAE (PyTorch, MNIST) â€” Student Notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
